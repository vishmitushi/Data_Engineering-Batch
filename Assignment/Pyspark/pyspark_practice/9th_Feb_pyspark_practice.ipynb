{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b195dde4-5f57-4a47-aac0-92eaf6ef9c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('DataFrame_operations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d30bbf2-e154-49b4-a169-90cea5882e01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n|Text_data_in_rows_using_format|\n+------------------------------+\n|          [work_year,job_ti...|\n|          [2023,Data, DevOp...|\n|          [2023,Data, Archi...|\n|          [2023,Data, Archi...|\n|          [2023,Data, Scien...|\n|          [2023,Data, Scien...|\n|          [2023,Data, Scien...|\n|          [2023,Data, Scien...|\n|          [2023,Machine, Le...|\n|          [2023,Machine, Le...|\n|          [2023,Data, Engin...|\n|          [2023,Data, Engin...|\n|          [2023,Machine, Le...|\n|          [2023,Machine, Le...|\n|          [2023,Data, Scien...|\n|          [2023,Data, Scien...|\n|          [2023,Data, Analy...|\n|          [2023,Data, Analy...|\n|          [2023,Data, Scien...|\n|          [2023,Data, Scien...|\n+------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_read_format = spark.read.format(\"text\").load(\"/FileStore/tables/Sample-1.txt\")\n",
    "\n",
    "# df_read_format.show()\n",
    " \n",
    "df_read_format.selectExpr(\"split(value,' ') as Text_data_in_rows_using_format\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67275dc8-ac17-4b92-ad48-157cff650a7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+\n|  Name|       DOB|Gender|salary|\n+------+----------+------+------+\n|   Ram|1991-04-01|     M|  3000|\n|  Mike|2000-05-19|     M|  4000|\n|Rohini|1978-09-05|     M|  4000|\n| Maria|1967-12-01|     F|  4000|\n| Jenis|1980-02-17|     F|  1200|\n+------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create data in dataframe\n",
    "data = [(('Ram'), '1991-04-01', 'M', 3000),\n",
    "       (('Mike'), '2000-05-19', 'M', 4000),\n",
    "       (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "       (('Maria'), '1967-12-01', 'F', 4000),\n",
    "       (('Jenis'), '1980-02-17', 'F', 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data ,schema=columns)\n",
    " \n",
    "# Print the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185aac25-8cc0-4850-a570-5eba914c1223",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+-------+\n|  Name|       DOB|Gender|salary|Country|\n+------+----------+------+------+-------+\n|   Ram|1991-04-01|     M|  3000|  India|\n|  Mike|2000-05-19|     M|  4000|  India|\n|Rohini|1978-09-05|     M|  4000|  India|\n| Maria|1967-12-01|     F|  4000|  India|\n| Jenis|1980-02-17|     F|  1200|  India|\n+------+----------+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_addColumn = df.withColumn('Country',lit('India'))\n",
    "df_addColumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0be060-cf2e-4275-9478-aebaeefd95e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+-------+------------+\n|  Name|       DOB|Gender|salary|Country|Name-Country|\n+------+----------+------+------+-------+------------+\n|   Ram|1991-04-01|     M|  3000|  India|   Ram-India|\n|  Mike|2000-05-19|     M|  4000|  India|  Mike-India|\n|Rohini|1978-09-05|     M|  4000|  India|Rohini-India|\n| Maria|1967-12-01|     F|  4000|  India| Maria-India|\n| Jenis|1980-02-17|     F|  1200|  India| Jenis-India|\n+------+----------+------+------+-------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df_concat = df_addColumn.withColumn(\"Name-Country\",concat_ws(\"-\",'Name','Country'))\n",
    "df_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8937b0ed-6254-4c2c-8737-14e9a3306c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+-------+\n|  Name|       DOB|Gender|salary|Country|\n+------+----------+------+------+-------+\n|   Ram|1991-04-01|     M|  3000|  India|\n|  Mike|2000-05-19|     M|  4000|  India|\n|Rohini|1978-09-05|     M|  4000|  India|\n| Maria|1967-12-01|     F|  4000|  India|\n| Jenis|1980-02-17|     F|  1200|  India|\n+------+----------+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "if 'Country' not in df.columns:\n",
    "    df.withColumn(\"Country\", lit('India')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509d8550-db9c-4782-999f-2abf9a855700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+\n|  Name|       DOB|Gender|salary|\n+------+----------+------+------+\n|   Ram|1991-04-01|     M|  3000|\n|  Mike|2000-05-19|     M|  4000|\n|Rohini|1978-09-05|     M|  4000|\n| Maria|1967-12-01|     F|  4000|\n| Jenis|1980-02-17|     F|  1200|\n+------+----------+------+------+\n\n+------+----------------+\n|Gender|Group_sum_Salary|\n+------+----------------+\n|     M|           11000|\n|     F|            5200|\n+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.show()\n",
    "# rename grouped column using select\n",
    "df.groupBy(\"Gender\").sum('salary').select(col('Gender'),col(\"sum(salary)\").alias('Group_sum_Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bec3a8-d13e-4e26-a560-5dce6b812f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+---------------+------+-------------+------------------+----------------+---------------+------------+----------------+------------+\n|work_year|           job_title|        job_category|salary_currency|salary|salary_in_usd|employee_residence|experience_level|employment_type|work_setting|company_location|company_size|\n+---------+--------------------+--------------------+---------------+------+-------------+------------------+----------------+---------------+------------+----------------+------------+\n|     2023|Data DevOps Engineer|    Data Engineering|            EUR| 88000|        95012|           Germany|       Mid-level|      Full-time|      Hybrid|         Germany|           L|\n|     2023|      Data Architect|Data Architecture...|            USD|186000|       186000|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n|     2023|      Data Architect|Data Architecture...|            USD| 81800|        81800|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n|     2023|      Data Scientist|Data Science and ...|            USD|212000|       212000|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n|     2023|      Data Scientist|Data Science and ...|            USD| 93300|        93300|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n|     2023|      Data Scientist|Data Science and ...|            USD|130000|       130000|     United States|          Senior|      Full-time|      Remote|   United States|           M|\n|     2023|      Data Scientist|Data Science and ...|            USD|100000|       100000|     United States|          Senior|      Full-time|      Remote|   United States|           M|\n|     2023|Machine Learning ...|Machine Learning ...|            USD|224400|       224400|     United States|       Mid-level|      Full-time|   In-person|   United States|           M|\n|     2023|Machine Learning ...|Machine Learning ...|            USD|138700|       138700|     United States|       Mid-level|      Full-time|   In-person|   United States|           M|\n|     2023|       Data Engineer|    Data Engineering|            USD|210000|       210000|     United States|       Executive|      Full-time|      Remote|   United States|           M|\n|     2023|       Data Engineer|    Data Engineering|            USD|168000|       168000|     United States|       Executive|      Full-time|      Remote|   United States|           M|\n|     2023|Machine Learning ...|Machine Learning ...|            USD|224400|       224400|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n|     2023|Machine Learning ...|Machine Learning ...|            USD|138700|       138700|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n|     2023|      Data Scientist|Data Science and ...|            GBP| 35000|        43064|    United Kingdom|       Mid-level|      Full-time|   In-person|  United Kingdom|           M|\n|     2023|      Data Scientist|Data Science and ...|            GBP| 30000|        36912|    United Kingdom|       Mid-level|      Full-time|   In-person|  United Kingdom|           M|\n|     2023|        Data Analyst|       Data Analysis|            USD| 95000|        95000|     United States|     Entry-level|      Full-time|   In-person|   United States|           M|\n|     2023|        Data Analyst|       Data Analysis|            USD| 75000|        75000|     United States|     Entry-level|      Full-time|   In-person|   United States|           M|\n|     2023|      Data Scientist|Data Science and ...|            USD|300000|       300000|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n|     2023|      Data Scientist|Data Science and ...|            USD|234000|       234000|     United States|          Senior|      Full-time|   In-person|   United States|           M|\n+---------+--------------------+--------------------+---------------+------+-------------+------------------+----------------+---------------+------------+----------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file = spark.read.csv('/FileStore/tables/table1/jobs_in_data.csv', sep = ',', inferSchema = True, header = True)\n",
    "csv_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c839fdd-2d77-40c7-9f14-ba07e0456c6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n |-- _c8: string (nullable = true)\n |-- _c9: string (nullable = true)\n |-- _c10: string (nullable = true)\n |-- _c11: string (nullable = true)\n\nroot\n |-- work_year: integer (nullable = true)\n |-- job_title: string (nullable = true)\n |-- job_category: string (nullable = true)\n |-- salary_currency: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- salary_in_usd: integer (nullable = true)\n |-- employee_residence: string (nullable = true)\n |-- experience_level: string (nullable = true)\n |-- employment_type: string (nullable = true)\n |-- work_setting: string (nullable = true)\n |-- company_location: string (nullable = true)\n |-- company_size: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv('/FileStore/tables/table1/jobs_in_data.csv')\n",
    "test.printSchema() # header = False, infer = False bu default\n",
    "csv_file.printSchema() # infer schema = True gives the data type to the columns according to the values and header = True is given to read first line as column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3926a1-39f4-4109-9fac-c4b6c1a4a610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|sum(salary)|\n+--------------------+-----------+\n|Machine Learning ...|     363100|\n|      Data Scientist|    1134300|\n|        Data Analyst|     170000|\n|Data DevOps Engineer|      88000|\n|      Data Architect|     267800|\n|Machine Learning ...|     363100|\n|       Data Engineer|     378000|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').sum('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0511667-770d-4ea9-bc54-f906329e9809",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|min(salary)|\n+--------------------+-----------+\n|Machine Learning ...|     138700|\n|      Data Scientist|      30000|\n|        Data Analyst|      75000|\n|Data DevOps Engineer|      88000|\n|      Data Architect|      81800|\n|Machine Learning ...|     138700|\n|       Data Engineer|     168000|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').min('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc9baf1-8d2c-4186-9e4a-a142d24da63d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|max(salary)|\n+--------------------+-----------+\n|Machine Learning ...|     224400|\n|      Data Scientist|     300000|\n|        Data Analyst|      95000|\n|Data DevOps Engineer|      88000|\n|      Data Architect|     186000|\n|Machine Learning ...|     224400|\n|       Data Engineer|     210000|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').max('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad73420a-1b7d-41fb-b888-116317e39431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|avg(salary)|\n+--------------------+-----------+\n|Machine Learning ...|   181550.0|\n|      Data Scientist|   141787.5|\n|        Data Analyst|    85000.0|\n|Data DevOps Engineer|    88000.0|\n|      Data Architect|   133900.0|\n|Machine Learning ...|   181550.0|\n|       Data Engineer|   189000.0|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').avg('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bf9f9e-5175-48d1-8f8b-64eaa3af604a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|avg(salary)|\n+--------------------+-----------+\n|Machine Learning ...|   181550.0|\n|      Data Scientist|   141787.5|\n|        Data Analyst|    85000.0|\n|Data DevOps Engineer|    88000.0|\n|      Data Architect|   133900.0|\n|Machine Learning ...|   181550.0|\n|       Data Engineer|   189000.0|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').mean('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae15f78f-77d2-4810-8e60-14ff7b14c5b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|           job_title|count|\n+--------------------+-----+\n|Machine Learning ...|    2|\n|      Data Scientist|    8|\n|        Data Analyst|    2|\n|Data DevOps Engineer|    1|\n|      Data Architect|    2|\n|Machine Learning ...|    2|\n|       Data Engineer|    2|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5c1a54-3446-451e-a1b5-80e9a2c5b115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----------+\n|employee_residence|           job_title|sum(salary)|\n+------------------+--------------------+-----------+\n|     United States|       Data Engineer|     378000|\n|     United States|Machine Learning ...|     363100|\n|     United States|Machine Learning ...|     363100|\n|    United Kingdom|      Data Scientist|      65000|\n|     United States|        Data Analyst|     170000|\n|     United States|      Data Scientist|    1069300|\n|     United States|      Data Architect|     267800|\n|           Germany|Data DevOps Engineer|      88000|\n+------------------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy(\"employee_residence\",\"job_title\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b94f06e-bc25-4414-bda3-dcc250fda978",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|sum(salary)|\n+-----------+\n|    2764300|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.agg({'salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bdc6f35-810f-4bb6-a9d5-7b2ebf3b7dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------+-------------+\n|           job_title|Germany|United Kingdom|United States|\n+--------------------+-------+--------------+-------------+\n|Machine Learning ...|   null|          null|            2|\n|      Data Scientist|   null|             2|            6|\n|        Data Analyst|   null|          null|            2|\n|Data DevOps Engineer|      1|          null|         null|\n|      Data Architect|   null|          null|            2|\n|Machine Learning ...|   null|          null|            2|\n|       Data Engineer|   null|          null|            2|\n+--------------------+-------+--------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy(\"job_title\").pivot(\"employee_residence\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb60db8-0121-448a-a9ef-c714341e95e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Architect| 81800|              null|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|                null| 93300|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|      Data Scientist|100000|     United States|      Remote|            null|\n|     null|Machine Learning ...|  null|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|        null|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|                null|168000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 95000|              null|   In-person|   United States|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n|     2023|                null|234000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null = spark.read.csv('/FileStore/tables/jobs_in_data___Copy.csv',header=True,inferSchema=True)\n",
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ca7598-656c-470e-aed7-d602e99f5f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269d577c-0d28-4120-9a9b-0b96c7be45c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Architect| 81800|              null|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|                null| 93300|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|      Data Scientist|100000|     United States|      Remote|            null|\n|     null|Machine Learning ...|  null|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|        null|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|                null|168000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 95000|              null|   In-person|   United States|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n|     2023|                null|234000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null.na.drop(how='all').show() # all values should be null to get removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04fda6c2-89df-4b6c-a6a2-6a3e34e63c0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Architect| 81800|              null|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|                null| 93300|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|      Data Scientist|100000|     United States|      Remote|            null|\n|     2023|Machine Learning ...|138700|     United States|        null|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|                null|168000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 95000|              null|   In-person|   United States|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n|     2023|                null|234000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null.na.drop(thresh=5).show() # atleast 5 non-null values should be there in row to be in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ce020f-a859-4da1-a77f-3084ed21086a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Architect| 81800|              null|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|      Data Scientist|100000|     United States|      Remote|            null|\n|     null|Machine Learning ...|  null|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|        null|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 95000|              null|   In-person|   United States|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null.na.drop(subset='job_title').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7651999c-e351-4ced-a06a-b943ee32eefd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+--------------+----------------+\n|work_year|           job_title|salary|employee_residence|  work_setting|company_location|\n+---------+--------------------+------+------------------+--------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|        Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|     In-person|   United States|\n|     2023|      Data Architect| 81800|    Missing Values|     In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|     In-person|   United States|\n|     2023|      Missing Values| 93300|     United States|     In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|        Remote|   United States|\n|     2023|      Data Scientist|100000|     United States|        Remote|  Missing Values|\n|     null|Machine Learning ...|  null|     United States|     In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|Missing Values|   United States|\n|     2023|       Data Engineer|210000|     United States|        Remote|   United States|\n|     2023|      Missing Values|168000|     United States|        Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|     In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|     In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|     In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|     In-person|  United Kingdom|\n|     2023|        Data Analyst| 95000|    Missing Values|     In-person|   United States|\n|     2023|        Data Analyst| 75000|     United States|     In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|     In-person|   United States|\n|     2023|      Missing Values|234000|     United States|     In-person|   United States|\n+---------+--------------------+------+------------------+--------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef82c1c3-be0a-44fe-8031-bced2c529b0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Architect| 81800|              null|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|                null| 93300|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|      Data Scientist|100000|     United States|      Remote|            null|\n|        0|Machine Learning ...|     0|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|        null|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|                null|168000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 95000|              null|   In-person|   United States|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n|     2023|                null|234000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a117aa-6e29-44b1-808a-fe15ef894e05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     null|Machine Learning ...|  null|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Architect| 81800|              null|   In-person|   United States|\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|                null| 93300|     United States|   In-person|   United States|\n|     2023|        Data Analyst| 95000|              null|   In-person|   United States|\n|     2023|      Data Scientist|100000|     United States|      Remote|            null|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|138700|     United States|        null|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|                null|168000|     United States|      Remote|   United States|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|                null|234000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_null.sort('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d461b0-163a-4a43-b122-6cc03b62775b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),\n",
    "       (2, \"Rose\",1 , \"2010\", \"20\",\"M\", 4000),\n",
    "       (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\n",
    "       (4, \"Jones\",2 ,\"2005\",\"10\",\"F\",2000),\n",
    "       (5,\"Brown\",2,\"2010\",\"40\",\"\",-1),\n",
    "       (6, \"Brown\", 2, \"2010\",\"50\",\"\",-1)]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show()\n",
    "\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62483260-c5c6-4b01-bea4-f55222377fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f231067a-7bdf-4f61-840a-dd7342979b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show()\n",
    "#Or instead of outer we can give full Or fullouter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce3d2eb-0877-495a-8bf8-beabd458e987",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show()\n",
    "# Or leftouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6699d38-7606-4037-bde2-bd2ccb0ec585",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show()\n",
    "# Or rightouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262bc87e-5721-403d-b1bb-2f2e186b6dda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0a29e7-0ce1-4e04-ad39-f22e8c194737",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|     6|Brown|              2|       2010|         50|      |    -1|\n+------+-----+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a161559-81d2-400a-b84e-eee872087b45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "9th_Feb_pyspark_practice",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
