{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5fee65-c4da-4fb6-9284-dfb0bb360fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+\n|   Name|Age|Gender|Salary|\n+-------+---+------+------+\n|Mitushi| 22|     F|  1000|\n|Vishesh| 24|     M|  2000|\n|Tanisha| 22|     F|  3000|\n| Zamran| 38|     M|  5000|\n+-------+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Execute Manipulating, Dropping, Sorting, Aggregations, Joining, GroupeBy in DataFrames\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initializing Spark Session\n",
    "spark = SparkSession.builder.appName(\"Manipulation in dataframes\").getOrCreate()\n",
    "# Creating dataframe\n",
    "data = [\n",
    "    ('Mitushi',22,'F',1000),\n",
    "    ('Vishesh',24,'M',2000),\n",
    "    ('Tanisha',22,'F',3000),\n",
    "    ('Zamran',38,'M',5000)\n",
    "]\n",
    "columns = [\"Name\",'Age','Gender',\"Salary\"]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25b4efb-66a9-4d78-9e14-c249d0fec13f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+-------+\n|   Name|Age|Gender|Salary|Country|\n+-------+---+------+------+-------+\n|Mitushi| 22|     F|  1000|  India|\n|Vishesh| 24|     M|  2000|  India|\n|Tanisha| 22|     F|  3000|  India|\n| Zamran| 38|     M|  5000|  India|\n+-------+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Execute Manipulating \n",
    "# Adding new column using withColumn() with lit()\n",
    "from pyspark.sql.functions import lit \n",
    "\n",
    "df_newcolumn = df.withColumn('Country',lit(\"India\"))\n",
    "df_newcolumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775be34a-0824-4c04-a501-d38a439ab1a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+\n|   Name|Age|Gender|Salary|\n+-------+---+------+------+\n|Vishesh| 24|     M|  2000|\n| Zamran| 38|     M|  5000|\n+-------+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering Data using filter()\n",
    "\n",
    "df_filter = df.filter(df['Age']> 22)\n",
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75138a0c-3640-45e9-9873-4b140af46734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|   Name|\n+-------+\n|Mitushi|\n|Vishesh|\n|Tanisha|\n| Zamran|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Selecting Columns\n",
    "df.select(df['Name']).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2481055-3d70-4ee4-88a6-d1b1230caadd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Architect| 81800|              null|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|                null| 93300|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|      Data Scientist|100000|     United States|      Remote|            null|\n|     null|Machine Learning ...|  null|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|        null|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|                null|168000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 95000|              null|   In-person|   United States|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n|     2023|                null|234000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Execute Dropping \n",
    "csv_file = spark.read.csv('/FileStore/tables/jobs_in_data___Copy.csv',header=True,inferSchema=True)\n",
    "csv_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d37baf-0428-4787-8b48-e33595a16f2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+----------------+\n|           job_title|salary|employee_residence|company_location|\n+--------------------+------+------------------+----------------+\n|Data DevOps Engineer| 88000|           Germany|         Germany|\n|      Data Architect|186000|     United States|   United States|\n|      Data Architect| 81800|              null|   United States|\n|      Data Scientist|212000|     United States|   United States|\n|                null| 93300|     United States|   United States|\n|      Data Scientist|130000|     United States|   United States|\n|      Data Scientist|100000|     United States|            null|\n|Machine Learning ...|  null|     United States|   United States|\n|Machine Learning ...|138700|     United States|   United States|\n|       Data Engineer|210000|     United States|   United States|\n|                null|168000|     United States|   United States|\n|Machine Learning ...|224400|     United States|   United States|\n|Machine Learning ...|138700|     United States|   United States|\n|      Data Scientist| 35000|    United Kingdom|  United Kingdom|\n|      Data Scientist| 30000|    United Kingdom|  United Kingdom|\n|        Data Analyst| 95000|              null|   United States|\n|        Data Analyst| 75000|     United States|   United States|\n|      Data Scientist|300000|     United States|   United States|\n|                null|234000|     United States|   United States|\n+--------------------+------+------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# dropping column\n",
    "csv_file.drop(csv_file['work_year'],csv_file['work_setting']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b469695-b1df-4d9b-8e3a-0b0ae3694d13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+------------------+------------+----------------+\n|work_year|           job_title|salary|employee_residence|work_setting|company_location|\n+---------+--------------------+------+------------------+------------+----------------+\n|     2023|Data DevOps Engineer| 88000|           Germany|      Hybrid|         Germany|\n|     2023|      Data Architect|186000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|212000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|130000|     United States|      Remote|   United States|\n|     2023|       Data Engineer|210000|     United States|      Remote|   United States|\n|     2023|Machine Learning ...|224400|     United States|   In-person|   United States|\n|     2023|Machine Learning ...|138700|     United States|   In-person|   United States|\n|     2023|      Data Scientist| 35000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|      Data Scientist| 30000|    United Kingdom|   In-person|  United Kingdom|\n|     2023|        Data Analyst| 75000|     United States|   In-person|   United States|\n|     2023|      Data Scientist|300000|     United States|   In-person|   United States|\n+---------+--------------------+------+------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# dropping na values\n",
    "csv_file.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796ca0de-5c66-4bba-bbf0-c959c980b4c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+\n|   Name|Age|Gender|Salary|\n+-------+---+------+------+\n| Zamran| 38|     M|  5000|\n|Vishesh| 24|     M|  2000|\n|Tanisha| 22|     F|  3000|\n|Mitushi| 22|     F|  1000|\n+-------+---+------+------+\n\n+-------+---+------+------+\n|   Name|Age|Gender|Salary|\n+-------+---+------+------+\n|Mitushi| 22|     F|  1000|\n|Tanisha| 22|     F|  3000|\n|Vishesh| 24|     M|  2000|\n| Zamran| 38|     M|  5000|\n+-------+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sorting\n",
    "# using sort()\n",
    "df.sort(df['Name'].desc()).show()\n",
    "# using orderBy()\n",
    "df.orderBy('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6f5d87-5f27-4657-a7a1-4d404ab0f39e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|sum(salary)|\n+--------------------+-----------+\n|                null|     495300|\n|Machine Learning ...|     363100|\n|      Data Scientist|     807000|\n|        Data Analyst|     170000|\n|Data DevOps Engineer|      88000|\n|      Data Architect|     267800|\n|Machine Learning ...|     138700|\n|       Data Engineer|     210000|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Aggregations\n",
    "csv_file.groupBy('job_title').sum('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab3d0375-59b1-4e60-a4b8-daa39507194f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|min(salary)|\n+--------------------+-----------+\n|                null|      93300|\n|Machine Learning ...|     138700|\n|      Data Scientist|      30000|\n|        Data Analyst|      75000|\n|Data DevOps Engineer|      88000|\n|      Data Architect|      81800|\n|Machine Learning ...|     138700|\n|       Data Engineer|     210000|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').min('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ef4a79-85ce-42db-be82-2a4db044deea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|           job_title|avg(salary)|\n+--------------------+-----------+\n|                null|   165100.0|\n|Machine Learning ...|   181550.0|\n|      Data Scientist|   134500.0|\n|        Data Analyst|    85000.0|\n|Data DevOps Engineer|    88000.0|\n|      Data Architect|   133900.0|\n|Machine Learning ...|   138700.0|\n|       Data Engineer|   210000.0|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').avg('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2573e9-9bf0-4a2d-8f11-0b17f1b29771",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|           job_title|count|\n+--------------------+-----+\n|                null|    3|\n|Machine Learning ...|    2|\n|      Data Scientist|    6|\n|        Data Analyst|    2|\n|Data DevOps Engineer|    1|\n|      Data Architect|    2|\n|Machine Learning ...|    2|\n|       Data Engineer|    1|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "csv_file.groupBy('job_title').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eaf809f-439e-4b55-8455-c18a9cd00517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Joins in pyspark\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),\n",
    "       (2, \"Rose\",1 , \"2010\", \"20\",\"M\", 4000),\n",
    "       (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\n",
    "       (4, \"Jones\",2 ,\"2005\",\"10\",\"F\",2000),\n",
    "       (5,\"Brown\",2,\"2010\",\"40\",\"\",-1),\n",
    "       (6, \"Brown\", 2, \"2010\",\"50\",\"\",-1)]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.show()\n",
    "\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ad1f28-9849-4e4e-a3ce-be6a5cc2ba9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b34c4b9-2eae-417a-8040-2f506fe206b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0c9f0e-8631-4e1d-bde3-fe6221f15987",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35164b06-a94e-47f6-89af-6e4afd1a991c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f444a9e4-8cd6-4bf9-ab82-3622456ca77a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26227c6-40cf-48f7-986e-a208c9f978de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|     6|Brown|              2|       2010|         50|      |    -1|\n+------+-----+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67808e6a-54ba-45a4-8350-c210da98c88f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Execute Pyspark - sparkSQL joins & Applying Functions in a Pandas DataFrame\n",
    "# Spark SQL joins\n",
    "empDF.createOrReplaceTempView(\"EmployeeView\")\n",
    "deptDF.createOrReplaceTempView(\"DeptView\")\n",
    "spark.sql(\"SELECT * from EmployeeView\").show()\n",
    "spark.sql(\"SELECT * from DeptView\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d625e9a9-eec2-4790-84ea-63b36919efd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|dept_name|dept_id|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|  Finance|     10|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|  Finance|     10|     3|Williams|              1|       2010|         10|     M|  1000|\n|  Finance|     10|     4|   Jones|              2|       2005|         10|     F|  2000|\n|Marketing|     20|     2|    Rose|              1|       2010|         20|     M|  4000|\n|       IT|     40|     5|   Brown|              2|       2010|         40|      |    -1|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from DeptView DeptDF JOIN EmployeeView EmpDF ON empDF.emp_dept_id ==  deptDF.dept_id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410617a7-834b-426e-9e0e-09550e9c6910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|dept_name|dept_id|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|  Finance|     10|     4|   Jones|              2|       2005|         10|     F|  2000|\n|  Finance|     10|     3|Williams|              1|       2010|         10|     M|  1000|\n|  Finance|     10|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|Marketing|     20|     2|    Rose|              1|       2010|         20|     M|  4000|\n|    Sales|     30|  null|    null|           null|       null|       null|  null|  null|\n|       IT|     40|     5|   Brown|              2|       2010|         40|      |    -1|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from DeptView DeptDF LEFT JOIN EmployeeView EmpDF ON empDF.emp_dept_id ==  deptDF.dept_id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa3abed-5675-491f-bb95-d2404685994a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|dept_name|dept_id|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|  Finance|     10|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|Marketing|     20|     2|    Rose|              1|       2010|         20|     M|  4000|\n|  Finance|     10|     3|Williams|              1|       2010|         10|     M|  1000|\n|  Finance|     10|     4|   Jones|              2|       2005|         10|     F|  2000|\n|       IT|     40|     5|   Brown|              2|       2010|         40|      |    -1|\n|     null|   null|     6|   Brown|              2|       2010|         50|      |    -1|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from DeptView DeptDF RIGHT JOIN EmployeeView EmpDF ON empDF.emp_dept_id ==  deptDF.dept_id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ac755f-5ebf-4d23-a69a-a8ca3e3bb5a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|dept_name|dept_id|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n|  Finance|     10|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|  Finance|     10|     3|Williams|              1|       2010|         10|     M|  1000|\n|  Finance|     10|     4|   Jones|              2|       2005|         10|     F|  2000|\n|Marketing|     20|     2|    Rose|              1|       2010|         20|     M|  4000|\n|    Sales|     30|  null|    null|           null|       null|       null|  null|  null|\n|       IT|     40|     5|   Brown|              2|       2010|         40|      |    -1|\n|     null|   null|     6|   Brown|              2|       2010|         50|      |    -1|\n+---------+-------+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from DeptView DeptDF FULL OUTER JOIN EmployeeView EmpDF ON empDF.emp_dept_id ==  deptDF.dept_id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466d4857-2c50-45a3-be5a-891558d15362",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|    Sales|     30|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from DeptView DeptDF LEFT ANTI JOIN EmployeeView EmpDF ON empDF.emp_dept_id ==  deptDF.dept_id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edc3f19-4b36-4351-8a12-d8b3093bd08b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from DeptView DeptDF LEFT SEMI JOIN EmployeeView EmpDF ON empDF.emp_dept_id ==  deptDF.dept_id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db88c61c-d1c5-45ac-a384-c79e434658d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying Functions in a Pandas DataFrame\n",
    "import pyspark.pandas as ps\n",
    "pandasdf = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pser):\n",
    "    return pser + 1 \n",
    "pandasdf.apply(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ebecee-bab6-455b-b0dd-fc2e49d55547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+----------+\n|   Name|Age|Gender|Salary|AgeSquared|\n+-------+---+------+------+----------+\n|Mitushi| 22|     F|  1000|     484.0|\n|Vishesh| 24|     M|  2000|     576.0|\n|Tanisha| 22|     F|  3000|     484.0|\n| Zamran| 38|     M|  5000|    1444.0|\n+-------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def square_udf(age_series: pd.Series) -> pd.Series:\n",
    "    return age_series.apply(lambda x: x ** 2)\n",
    "\n",
    "# Apply the Pandas UDF to a column\n",
    "df = df.withColumn(\"AgeSquared\", square_udf(df[\"Age\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a9720e9-32c0-4b10-bb26-6e258fba8931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Coding_Challenge",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
